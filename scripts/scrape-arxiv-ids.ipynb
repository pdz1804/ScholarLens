{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-08T08:53:12.918569Z",
     "iopub.status.busy": "2025-08-08T08:53:12.918314Z",
     "iopub.status.idle": "2025-08-08T08:53:21.394699Z",
     "shell.execute_reply": "2025-08-08T08:53:21.393470Z",
     "shell.execute_reply.started": "2025-08-08T08:53:12.918549Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selectolax\n",
      "  Downloading selectolax-0.3.32-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (6.2 kB)\n",
      "Downloading selectolax-0.3.32-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: selectolax\n",
      "Successfully installed selectolax-0.3.32\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "!pip install selectolax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T08:53:21.397040Z",
     "iopub.status.busy": "2025-08-08T08:53:21.396311Z",
     "iopub.status.idle": "2025-08-08T08:53:21.554388Z",
     "shell.execute_reply": "2025-08-08T08:53:21.553405Z",
     "shell.execute_reply.started": "2025-08-08T08:53:21.397008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from selectolax.parser import HTMLParser\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T08:53:21.557429Z",
     "iopub.status.busy": "2025-08-08T08:53:21.557158Z",
     "iopub.status.idle": "2025-08-08T08:53:21.562257Z",
     "shell.execute_reply": "2025-08-08T08:53:21.561528Z",
     "shell.execute_reply.started": "2025-08-08T08:53:21.557409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "VALID_SHOW_VALUES = [25, 50, 100, 250, 500, 1000, 2000]\n",
    "\n",
    "HEADERS = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/115.0.0.0 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T08:53:21.563897Z",
     "iopub.status.busy": "2025-08-08T08:53:21.563459Z",
     "iopub.status.idle": "2025-08-08T08:53:21.582994Z",
     "shell.execute_reply": "2025-08-08T08:53:21.581952Z",
     "shell.execute_reply.started": "2025-08-08T08:53:21.563873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_arxiv_metadata(paper_url: str, paper_id: str, domain: str = \"cs\") -> dict:\n",
    "    \"\"\"\n",
    "    Scrapes metadata from a single arXiv abstract page.\n",
    "\n",
    "    Returns:\n",
    "        dict: Paper metadata with error handling for missing fields.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(paper_url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        html = HTMLParser(response.text)\n",
    "        content = html.css_first(\"div#content-inner\")\n",
    "\n",
    "        # Date\n",
    "        try:\n",
    "            date_text = content.css_first('div.dateline').text(strip=True)\n",
    "            date_submitted = date_text.replace('[Submitted on ', '').replace(']', '')\n",
    "        except Exception:\n",
    "            date_submitted = None\n",
    "\n",
    "        # Title\n",
    "        try:\n",
    "            title = content.css_first('h1.title').text(strip=True).replace('Title:', '').strip()\n",
    "        except Exception:\n",
    "            title = None\n",
    "\n",
    "        # Authors\n",
    "        try:\n",
    "            authors = content.css_first(\"div.authors\").text(strip=True).replace('Authors:', '').strip()\n",
    "        except Exception:\n",
    "            authors = None\n",
    "\n",
    "        # Abstract\n",
    "        try:\n",
    "            abstract = content.css_first(\"blockquote.abstract\").text(strip=True).replace('Abstract:', '').strip()\n",
    "        except Exception:\n",
    "            abstract = None\n",
    "\n",
    "        # Subjects\n",
    "        try:\n",
    "            raw_subjects = content.css_first('td.subjects').text(strip=True)\n",
    "            subjects = [s.strip().rsplit('(', 1)[0].strip() for s in raw_subjects.split(';')]\n",
    "        except Exception:\n",
    "            subjects = []\n",
    "\n",
    "        primary_subject = subjects[0] if len(subjects) != 0 else None\n",
    "\n",
    "        pdf_url = paper_url.replace('abs', 'pdf').strip()\n",
    "\n",
    "        return {\n",
    "            'Paper Title': title,\n",
    "            'Paper ID': paper_id,\n",
    "            'Authors': authors,\n",
    "            'Abstract': abstract,\n",
    "            'Domain': domain,\n",
    "            'Primary Subject': primary_subject,\n",
    "            'Subjects': subjects,\n",
    "            'Date Submitted': date_submitted,\n",
    "            'Abstract URL': paper_url,\n",
    "            'PDF URL': pdf_url\n",
    "        }\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed for {paper_url}: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T08:53:21.584658Z",
     "iopub.status.busy": "2025-08-08T08:53:21.584342Z",
     "iopub.status.idle": "2025-08-08T08:53:21.607452Z",
     "shell.execute_reply": "2025-08-08T08:53:21.606438Z",
     "shell.execute_reply.started": "2025-08-08T08:53:21.584637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def scrape_arxiv_list_page(list_url: str, max_papers: int = 3000, show: int = 100, domain: str = \"cs\") -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Iterates over multiple arXiv listing pages using pagination to collect metadata.\n",
    "\n",
    "#     Parameters:\n",
    "#         list_url (str): Base URL without skip parameter but including &show=...\n",
    "#         max_papers (int): Max number of papers to scrape\n",
    "#         show (int): Number of papers to show per page (25–2000)\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Extracted paper metadata\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "#     skip = 71000\n",
    "\n",
    "#     while len(results) < max_papers:\n",
    "#         paged_url = f\"{list_url}&skip={skip}\"\n",
    "#         print(f\"Scraping: {paged_url}\")\n",
    "\n",
    "#         response = requests.get(paged_url, headers=HEADERS)\n",
    "#         if response.status_code != 200:\n",
    "#             print(f\"[ERROR] Failed to fetch: {paged_url}\")\n",
    "#             break\n",
    "\n",
    "#         html = HTMLParser(response.text)\n",
    "#         papers = html.css('dl#articles > dt')\n",
    "\n",
    "#         print(f\"Number of papers: {len(papers)}\")\n",
    "#         if not papers:\n",
    "#             print(\"[INFO] Empty page — stopping.\")\n",
    "#             break\n",
    "\n",
    "#         for paper in tqdm(papers, desc=f\"Processing page starting at skip={skip}\"):\n",
    "#             if len(results) >= max_papers:\n",
    "#                 break\n",
    "#             abstract_node = paper.css_first('a[title=\"Abstract\"]')\n",
    "#             if abstract_node:\n",
    "#                 abstract_href = abstract_node.attributes.get('href')\n",
    "#                 full_url = f\"https://arxiv.org{abstract_href}\"\n",
    "#                 metadata = extract_arxiv_metadata(full_url, abstract_href.strip('/abs/'), domain=domain)\n",
    "#                 if metadata:\n",
    "#                     results.append(metadata)\n",
    "#             else:\n",
    "#                 print(\"[WARNING] Abstract link not found.\")\n",
    "\n",
    "#             time.sleep(1)\n",
    "\n",
    "\n",
    "#         skip -= show  # advance by the expected batch size, not len(papers)\n",
    "\n",
    "#     return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T08:53:21.608877Z",
     "iopub.status.busy": "2025-08-08T08:53:21.608554Z",
     "iopub.status.idle": "2025-08-08T08:53:21.632519Z",
     "shell.execute_reply": "2025-08-08T08:53:21.631515Z",
     "shell.execute_reply.started": "2025-08-08T08:53:21.608847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def scrape_arxiv_list_ids(list_url: str, max_papers: int = 1000, show: int = 100, domain: str = \"cs\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Iterates over multiple arXiv listing pages using pagination to collect metadata.\n",
    "\n",
    "    Parameters:\n",
    "        list_url (str): Base URL without skip parameter but including &show=...\n",
    "        max_papers (int): Max number of papers to scrape\n",
    "        show (int): Number of papers to show per page (25-2000)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted paper metadata\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    skip = 50000\n",
    "\n",
    "    while len(results) < max_papers:\n",
    "        paged_url = f\"{list_url}&skip={skip}\"\n",
    "        print(f\"Scraping: {paged_url}\")\n",
    "\n",
    "        response = requests.get(paged_url, headers=HEADERS)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"[ERROR] Failed to fetch: {paged_url}\")\n",
    "            break\n",
    "\n",
    "        html = HTMLParser(response.text)\n",
    "        papers = html.css('dl#articles > dt')\n",
    "\n",
    "        print(f\"Number of papers: {len(papers)}\")\n",
    "        if not papers:\n",
    "            print(\"[INFO] Empty page — stopping.\")\n",
    "            break\n",
    "\n",
    "        for paper in tqdm(papers, desc=f\"Processing page starting at skip={skip}\"):\n",
    "            if len(results) >= max_papers:\n",
    "                break\n",
    "            abstract_node = paper.css_first('a[title=\"Abstract\"]')\n",
    "            if abstract_node:\n",
    "                abstract_href = abstract_node.attributes.get('href')\n",
    "                if abstract_href:\n",
    "                    paper_id = abstract_href.replace(\"/abs/\", \"\").strip()\n",
    "                    abstract_url = f\"https://arxiv.org{abstract_href.strip()}\"\n",
    "                    results.append({\n",
    "                        \"paper_id\": paper_id,\n",
    "                        \"abstract_url\": abstract_url\n",
    "                    })\n",
    "            else:\n",
    "                print(\"[WARNING] Abstract link not found for a paper.\")\n",
    "\n",
    "            time.sleep(0.25)\n",
    "\n",
    "\n",
    "        skip += show  # advance by the expected batch size, not len(papers)\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T08:53:21.634084Z",
     "iopub.status.busy": "2025-08-08T08:53:21.633674Z",
     "iopub.status.idle": "2025-08-08T08:53:21.655693Z",
     "shell.execute_reply": "2025-08-08T08:53:21.654821Z",
     "shell.execute_reply.started": "2025-08-08T08:53:21.634051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def retrieve_arxiv_papers(domain: str = \"cs\", show: int = 500, max_papers: int = 1000, year = 2024) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validates input and initiates the arXiv scraping process.\n",
    "\n",
    "    Parameters:\n",
    "        domain (str): arXiv domain like 'cs.LG', 'cs.CL'\n",
    "        show (int): Number of papers per page (25–2000)\n",
    "        max_papers (int): Maximum total papers to retrieve\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted arXiv paper metadata\n",
    "    \"\"\"\n",
    "    if show not in VALID_SHOW_VALUES:\n",
    "        raise ValueError(f\"'show' must be one of {VALID_SHOW_VALUES}\")\n",
    "\n",
    "    base_url = f'https://arxiv.org/list/{domain}/{year}?show={show}'\n",
    "    return scrape_arxiv_list_ids(base_url, max_papers=max_papers, show=show, domain=domain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T08:53:21.657096Z",
     "iopub.status.busy": "2025-08-08T08:53:21.656709Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://arxiv.org/list/cs/2023?show=500&skip=50000\n",
      "Number of papers: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing page starting at skip=50000:  18%|█▊        | 91/500 [00:22<01:43,  3.95it/s]"
     ]
    }
   ],
   "source": [
    "df = retrieve_arxiv_papers(\"cs\", show=500, year=2023, max_papers=50000)\n",
    "# df = scrape_arxiv_list_page(url)\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "df.to_csv(r'arxiv_cs_2023_remaining.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
